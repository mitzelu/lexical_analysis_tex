\addtocontents{toc}{\protect\newpage}

\section{Implementation Part}

\pagenumbering{arabic}
\setcounter{page}{39}
\phantomsection
In the previous chapters the concepts and architecture of Finduber project were discussed. What follows now is the description of the implementation part. It will be analysed and described every step of the implementation process, together with the used technologies and code snippets. 

The whole part of the project is written in Python. Thus, frameworks and libraries, as described in Figure \ref{component_uml}, are Python related.

As the thesis is based on lexical analysis, the main two parts of its are pre-run console applications.

\subsection{Taxonomy Database}

The main two ideas were to use the hypernym/hyponym classification or wikipedia category tree. After some research it turns out that first approach is not suitable for my purpose, because defined categories has abstract words, like: beauty, fashion. Thus, the hypernym/hyponym tree of such a word will contain only abstract words like: appearance, attractiveness, handsomeness. In this way, it will never categorize term -- soap -- to industry -- beauty. 

Second method is more conveinent for Finduber project. Wikepia is the biggest online dictionary and contains over 473K unuque categories and over 995K edges in the categories taxonomic tree. Basiccaly, every word can be found on it page. Moreover, every article has one or more categories to which it belongs and each category a list of subcategories. That is what it needs so far. 

For a better understanding here is an example of a edge of wikipedia category tree:

\begin{enumerate}
\item[--] Beauty
\begin{enumerate}
\item[--] Skin care
\begin{enumerate}
\item[--] Sun tanning
\end{enumerate}
\end{enumerate}
\end{enumerate}

Now, if the given word has one of these categories it is classified as of Beauty type. 

For storing the category tree Finduber project uses Mongo database, because it has a simple structure of document hierarchy. This feature of nested documents will simplfy the search process. 

To retreive desired information from wikipedia page it is need first to extract the raw page and then to connect to Wikipedia API. In Python the library beautifulSoup is used to extract data out of HTML pages. However, to be able to extract only desired information out of a full text page it is need to examine the structure of raw HTML. What kind of tags, attributes and references are used.

Wikipedia pages has a simple HTML structure. The data about categories is enclosed in 'a' and 'div' tags of class 'mw-normal-catlinks'. Further, the library is doing all the job. Listing \ref{wiki_extractor} shows the full method described.

\lstinputlisting [language=Python, caption={Wikipedia Extractor}, label=wiki_extractor]{../src/wikipedia_extractor.py}

The method returns a list of categories. The next step is to define method that will use Wikipedi API to get the subcategories as deeper as required. MediaWiki action API is the web service that provides convenient access to wiki features and data over HTTP. For this is need to request a particular 'action' in the 'action' parameter. In Finduber project the action is set as query. Because we are looking for categories, other query parameters that need to be set are: llist -- categorymembers (list of pages that belong to given category); cmtitle -- the given category; cmtype -- type of category members to include.

The listing \ref{wiki_query} contains all logic for connection to MediaWiki. 

\lstinputlisting [language=Python, caption={Wikipedia Query}, label=wiki_query]{../src/wiki_query.py}

It takes as argument query\_params, that are specified in next method from listing \ref{wiki_parser}

\lstinputlisting [language=Python, caption={Wikipedia Parser}, label=wiki_parser]{../src/category_members.py}

Because the wikipedia HTML page uses to specify categories as 'Category: name' it is need first to append the string 'Category' to the given word. Category tree in wikipedia is in circular dependence. Thus, at some point it can get that word Biology can be part of Chicago Stags coaches category. To avoid this types of situations a maximum depth is set.

Final step is to store data in the MongoDB model. At this point the database of categories is set up. 

\subsection{Database Optimization using Lexical Analysis}

This part of the project is the main task that should be implemented in order to get the desired results of Finduber project. It represents also a console application that is meant to group database records in pre-defined categories. For doing this it is need to set up an working plan. The crucial aspects that should be covered are: 

\begin{itemize}
\item[--] Defining the database records that are relevant for text mining process.

\item[--] Analysing methods of database accessing to get the data.

\item[--] Implementing the pre-processing algorithm described in Section \ref{ssec:lexical} using technologies that work with big data.
\end{itemize}

Grouping a set of objects in such a way that objects in the same group are more similar to each others that to those in other group in machine learning is called cluster analysis. Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them.

The notion of a ``cluster" cannot be precisely defined, which is one of the reasons why there are so many clustering algorithms. Each of them employs different cluster models. Only understanding these cluster models, the difference between algorithms can be understood. This research is important for Finduber project, because it will help to choose the most suitable algorithm. Typical cluster models include:

\begin{enumerate}

\item[--] Connectivity models. The hierarchical clustering builds models based on distance connectivity.

\item[--] Centroid models. For example, the k-means algorithm represents each cluster by a single mean vector.

\item[--] Distribution models. Clusters are modeled using statistical distributions, such as multivariate normal distributions used by the term frequnecy algorithm. 

\item[--] Subspace models: in Biclustering (also known as Co-clustering or two-mode-clustering), clusters are modeled with both cluster members and relevant attributes.

\item[--] Group models: some algorithms do not provide a refined model for their results and just provide the grouping information.

\end{enumerate}

A set of such clusters represents a clustering. It is also important to determine what type of clustering should be used: hard or soft. Hard clustering means that each object belongs to a cluster or not. In soft clustering object belongs to each cluster to a certain degree (for example, a likelihood of belonging to the cluster). 

There are also a set of other characteristings that should be examinated: 

\begin{enumerate}
\item[--] strict partitioning clustering -- each object belongs to exactly one cluster;

\item[--] strict partitioning clustering with outliers -- objects can also belong to no cluster, and are considered outliers.

\item[--] overlapping clustering (also: alternative clustering, multi-view clustering): while usually a hard clustering, objects may belong to more than one cluster.

\item[--] hierarchical clustering: objects that belong to a child cluster also belong to the parent cluster
\end{enumerate}

Finduber system will use hard clustering and strict partitioning clustering with outliers. These two types of clustering are chosen, because the data should be classified in a fix and pre-defined number of categories. Now the problem is to find the best algorithm that will solve the problem. To do this below is provided a deeper analysis of some clustering algorithms that can be used. 

Hierarchical clustering algorithm builds a cluster hierarchy or, in other words, a tree of clusters. Every cluster node contains child clusters; sibling clusters partition the points covered by their common parent. Hierarchical clustering methods are categorized
into agglomerative (bottom-up) and divisive (top-down). This algorithm is not suitable for Finduber project, because it do not have nested categories.

K-means clustering is a method of vector quantization. It aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. In particular, the parameter k is known to be hard to choose when not given by external constraints. The algorithm has many limitations that make it powerless to solve the clustering problem of Finduber system. One of the limitation is that k-means algorithm cannot be used on non-numeerical data. The algorithm has problems when clusters are of difering sizes and densities, also when the data contains outliters. 

So far, the algorithms that use connectivity models and centroid models are not suitable for solving the data categorization problem defined in Finduber project. Distribution models algorithm is the right option. 

Term frequency is a statistical measure used to evaluate how important a word is to a corpus. The importance increases proportionally to the number of times a word appears in the document. The accuracy of the algorithm depends on the quality of chosen corpus and on the defined feature extraction. 

Text corpus represents a large and structured set of texts that is used to do statistical analysis. To create the corpus for Finduber project it is need to examine the database schema from listing \ref{schema} and to see which records are more relevant. 

\lstinputlisting [language=Ruby, caption={Database Schema}, label=schema]{../src/schema.rb}

Columns: title, description and keywords are chosen for the text corpus. Given that the database has over one million records the corpus set will represent a large data set. Processing and analysing it may take a lot of time. To solve the issue the MapReduce framework is used. 

MapReduce is an implementation for processing and generating lage data sets. It consists of an workflow and a distributed algorithm. To write a MapReduce workflow it is need to create two scripts: the map script, and the recude script. 

The map script takes some input data, and maps it to \textless key, value \textgreater pairs according to the specifications. For Finduber project, teh task is to count word frequnecies in a text, thus \textless word, count \textgreater will represents \textless key, value \textgreater pairs. The map script, then would emit a\textless word, 1 \textgreater pair for each word in the input stream. Aggregatio  (i.e. actual counting) is not the responsability of the map script, its goal is to model the data into \textless key, value \textgreater pairs for the reducer to aggregate. In the listing \ref{mapfunction} is shown the Map script with all specifications used in the project.

\lstinputlisting [language=Python, caption={Map Function}, label=mapfunction]{../src/map.py}

Emitted \textless key, value \textgreater pairs are then ``shuffled", that basically means that pairs with the same key are grouped and passed to a single machine, which will then run the reduce script over them.

One important specification of map function from listing \ref{mapfunction} is to filter the given corpus from stop-words, punctuation and to performe stemming. For this first it creates the corpus, as was mentioned about, the corpus consists of title and description. The corpus is passed as an argument to filt function represented in listing \ref{filtfunction}. The map function returns a dictionary that have as key the username of the user, that is used as an identifier. As values has a list of corpus tokens. 

\lstinputlisting [language=Python, caption={Filter Function}, label=filtfunction]{../src/filt.py}

The reduce script takes a collection of \textless key, value \textgreater pairs from the returned dictionary and ``reduces" them according to the specified reduce script. In Finduber project the task is to count the number of word occurrences to get frequencies. Thus, the reduce script will simply sum the values of the collection of \textless key, value \textgreater pairs which have the same key. Another responsability is to sort the words by their frequencies and to keep only the top four. The script is show in listing \ref{reducefunction}

\lstinputlisting [language=Python, caption={Reduce Function}, label=reducefunction]{../src/reduce.py}

The distributed algorithm orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications an data transfers between the parts of the system. Python has it own implemenation of the MapReduce distributed computing framework called mincemeat. The framework is: fault tolerant -- clients can join or leave the cluster at any time without affecting the entire process; secure -- it authenticates both ends of every connection, ensuring that only authorized code is executed. Mincemeat is an open source distributed under the MIT Lincese , that is why I used it for Finduber project. The declaration how to pass the data and map-reduce scripts to server is shown in listing \ref{startserver}

\lstinputlisting [language=Python, caption={Start MapReduce Server}, label=startserver]{../src/server.py}

Finduber project has to process and analyse using MapReduce framework 1 million database records that represent 300 MB of data. The text corpus was store in an text file for faster processing. For this the power of the PostreSQL was used. PostgreSQL has the functionality copy from the databse only required columns. As was specified text corpus for Finduber project consits of the columns: title and description. In the listing \ref{psqlcopy} is the PostreSQL command that was used. The id column is used for identification purpose.

\lstinputlisting [language=Python, caption={Command to Text Corpus}, label=psqlcopy]{../src/copypsql.py}

The corpus is ready, the framework and map-reduce scripts are created, it remains to analyse one more thing. How to update the records according to the results. Database has an empty column called -- category. After that every record will be categorised it is necessary to update the category column in order to keep the results. To update the category column during execution of MapReduce framework using sql queries the process will take a long time. It was reached this conclusion after a test run of the elaborated algorithm. The test was running on a limited number of records -- 100000. The execution time was 7 hours. This means that for entire databse of 1 million records the amunt of time will be 10 time bigger -- 70 hours. For reducing the execution time Finduber project has another method of updating the PostgreSQL database. The results from the MapReduce framework are stored in a csv file as shown in listing \ref{saveresults}.

\lstinputlisting [language=Python, caption={Storing the Results}, label=saveresults]{../src/output.py}

In first part of the listing the script checks to wich category the given result from the MapReduce server belongs. Second part stores the data in a csv file. Using these approach of saving the results, the execution time of the MapReduce was 1 hour and 5 minutes. The improvement is considerable. Having the file with the user title and its category other functionalities of PostgreSQL can be used to update the database. According to the file is created a temporary table using command shown in listing \ref{temporarfile}

\lstinputlisting [language=Python, caption={Create a Temporary Table in PostreSQL}, label=temporarfile]{../src/temp.py}

Now the last step is to update the database using this temporary table as specifed in listing \ref{updatetable}. After this the temporary file will be automatically deleted from the database. These is the last step in the process of database optimization using lexical analysis. 

\lstinputlisting [language=Python, caption={Update Database using Temporary Table}, label=updatetable]{../src/updatetemp.py}


The results of database optimisation shown in the Table \ref{table:tf_results} are not as high as expected. It happened beacuse of text corpus. Only channel's title and description is not enought for analysis. Thus, a feature implementation of the project is to add video titles and video description. 

\begin{table}[!ht]
\begin{center}
\caption{Result of term frequency algorithm}
\renewcommand{\arraystretch}{2}
\begin{tabular}{| c | >{\centering\arraybackslash}p{4cm} | }
\hline
\textbf{Category}& \textbf{Number of users} \\
\hline
Beauty & 2407 \\
\hline
Games & 77720 \\
\hline
Fashion & 554 \\
\hline
Food & 697 \\
\hline
Sports & 1263 \\
\hline
\hline
\textbf{Total} & 8264 \\
\hline
\end{tabular}
\label{table:tf_results}
\vspace{-2.5em}
\end{center}
\end{table}


\subsection{Web User Interface}

Web user interface represents a simple web page that has a search box where user is required to type the product that is need to be promoted. Since the Finduber project uses multiple libraries of Python language it is a good practice to use an virtual environment for developimng the application.

Virtualenv is a tool to create isolated Python environments. It solves the problem that appears when the same machine has an application that needs version one of a python library and another applicaion that requires version 2. If everything is installed into 
/usr/lib/python2.7/site-packages, at a moment library one for first application can be upgraded to version two. This fact can break the application, becaus any changes in its libraries or the versions of its are hazardous. 

Flask is used as a primary web framework, because it aims to keep to keep the core simple but extensible. Flask wonâ€™t make many decisions for the developer, such as what database to use. Those decisions that it does make, such as what templating engine to use, are easy to change. Everything else is up to the developer, so that Flask can be everything he needs and nothing he doesn not.

By default, Flask does not include a database abstraction layer, form validation or anything else where different libraries already exist that can handle that. For Finduber project was used the library Flask-SQLAlchemy. It is an extension that adds support for SQLAlchemy to application. The library is easy to use and has many functionalities. To make the connection to the PostgreSQL databse it is required to set the database url and to add it to configuration file, as shown in listing \ref{alchemyDB}

\lstinputlisting [language=Python, caption={SQLAlchemy Configuration}, label=alchemyDB]{../src/sqlalchemy.py}

In order to create a web page it is only necessary to create a standard python function. The listing \ref{approute} show how is created the web page index of Finduber project. It is not listed the entire code of the function, but only the declaration.

\lstinputlisting [language=Python, caption={Index Page in Flask}, label=approute]{../src/index.py}

The route () decorator tells Flask what URL should trigger the function. 

After pressing the search button a list of 10 influencial bloggers sorted by number of views is returned. 

In Figure \ref{web_app} is an example of user search. As a produs is taken a soap and the 10 influencial bloggers that are received are from beauty category.

\begin{figure}[!ht]
\centering
\includegraphics[width=15cm]{web}
\caption{Application Results}\label{web_app}
\end{figure}






