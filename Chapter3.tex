\section{Implementation Part}
\phantomsection
In the previous chapters the concepts and architecture of Finduber project were discussed. What follows now is the description of the implementation part. It will be analysed and described every step of the implementation process, together with the used technologies and code snippets. 

The whole part of the project is written in Python. Thus, frameworks and libraries, as described in Figure \ref{component_uml}, are Python related.

As the thesis is based on lexical analysis, the main two parts of its are pre-run console applications.

\subsection{Taxonomy Database}

The main two ideas were to use the hypernym/hyponym classification or wikipedia category tree. After some research it turns out that first approach is not suitable for my purpose, because defined categories has abstract words, like: beauty, fashion. Thus, the hypernym/hyponym tree of such a word will contain only abstract words like: appearance, attractiveness, handsomeness. In this way, it will never categorize term -- soap -- to industry -- beauty. 

Second method is more conveinent for Finduber project. Wikepia is the biggest online dictionary and contains over 473K unuque categories and over 995K edges in the categories taxonomic tree. Basiccaly, every word can be found on it page. Moreover, every article has one or more categories to which it belongs and each category a list of subcategories. That is what it needs so far. 

For a better understanding here is an example of a edge of wikipedia category tree:

\begin{enumerate}
\item[--] Beauty
\begin{enumerate}
\item[--] Skin care
\begin{enumerate}
\item[--] Sun tanning
\end{enumerate}
\end{enumerate}
\end{enumerate}

Now, if the given word has one of these categories it is classified as of Beauty type. 

For storing the category tree Finduber project uses Mongo database, because it has a simple structure of document hierarchy. This feature of nested documents will simplfy the search process. 

To retreive desired information from wikipedia page it is need first to extract the raw page and then to connect to Wikipedia API. In Python the library beautifulSoup is used to extract data out of HTML pages. However, to be able to extract only desired information out of a full text page it is need to examine the structure of raw HTML. What kind of tags, attributes and references are used.

Wikipedia pages has a simple HTML structure. The data about categories is enclosed in 'a' and 'div' tags of class 'mw-normal-catlinks'. Further, the library is doing all the job. Listing \ref{wiki_extractor} shows the full method described.

\lstinputlisting [language=Python, caption={Wikipedia Extractor}, label=wiki_extractor]{../src/wikipedia_extractor.py}

The method returns a list of categories. The next step is to define method that will use Wikipedi API to get the subcategories as deeper as required. MediaWiki action API is the web service that provides convenient access to wiki features and data over HTTP. For this is need to request a particular 'action' in the 'action' parameter. In Finduber project the action is set as query. Because we are looking for categories, other query parameters that need to be set are: llist -- categorymembers (list of pages that belong to given category); cmtitle -- the given category; cmtype -- type of category members to include.

The listing \ref{wiki_query} contains all logic for connection to MediaWiki. 

\lstinputlisting [language=Python, caption={Wikipedia Query}, label=wiki_query]{../src/wiki_query.py}

It takes as argument query\_params, that are specified in next method from listing \ref{wiki_parser}

\lstinputlisting [language=Python, caption={Wikipedia Parser}, label=wiki_parser]{../src/category_members.py}

Because the wikipedia HTML page uses to specify categories as 'Category: name' it is need first to append the string 'Category' to the given word. Category tree in wikipedia is in circular dependence. Thus, at some point it can get that word Biology can be part of Chicago Stags coaches category. To avoid this types of situations a maximum depth is set.

Final step is to store data in the MongoDB model. At this point the database of categories is set up. 

\subsection{Database Optimization using Lexical Analysis}

This part of the project is the main task that should be implemented in order to get the desired results of Finduber project. It represents also a console application that is meant to group database records in pre-defined categories. For doing this it is need to set up an working plan. The crucial aspects that should be covered are: 

\begin{itemize}
\item[--] Defining the database records that are relevant for text mining process.

\item[--] Analysing methods of database accessing to get the data.

\item[--] Implementing the pre-processing algorithm described in Section \ref{ssec:lexical} using technologies that work with big data.
\end{itemize}

Grouping a set of objects in such a way that objects in the same group are more similar to each others that to those in other group in machine learning is called cluster analysis. Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them.

The notion of a ``cluster" cannot be precisely defined, which is one of the reasons why there are so many clustering algorithms. Each of them employs different cluster models. Only understanding these cluster models, the difference between algorithms can be understood. This research is important for Finduber project, because it will help to choose the most suitable algorithm. Typical cluster models include:

\begin{enumerate}

\item[--] Connectivity models. The hierarchical clustering builds models based on distance connectivity.

\item[--] Centroid models. For example, the k-means algorithm represents each cluster by a single mean vector.

\item[--] Distribution models. Clusters are modeled using statistical distributions, such as multivariate normal distributions used by the term frequnecy algorithm. 

\item[--] Subspace models: in Biclustering (also known as Co-clustering or two-mode-clustering), clusters are modeled with both cluster members and relevant attributes.

\item[--] Group models: some algorithms do not provide a refined model for their results and just provide the grouping information.

\end{enumerate}

A set of such clusters represents a clustering. It is also important to determine what type of clustering should be used: hard or soft. Hard clustering means that each object belongs to a cluster or not. In soft clustering object belongs to each cluster to a certain degree (for example, a likelihood of belonging to the cluster). 

There are also a set of other characteristings that should be examinated: 

\begin{enumerate}
\item[--] strict partitioning clustering -- each object belongs to exactly one cluster;

\item[--] strict partitioning clustering with outliers -- objects can also belong to no cluster, and are considered outliers.

\item[--] overlapping clustering (also: alternative clustering, multi-view clustering): while usually a hard clustering, objects may belong to more than one cluster.

\item[--] hierarchical clustering: objects that belong to a child cluster also belong to the parent cluster
\end{enumerate}

Finduber system will use hard clustering and strict partitioning clustering with outliers. These two types of clustering are chosen, because the data should be classified in a fix and pre-defined number of categories. Now the problem is to find the best algorithm that will solve the problem. To do this below is provided a deeper analysis of some clustering algorithms that can be used. 

Hierarchical clustering algorithm builds a cluster hierarchy or, in other words, a tree of clusters. Every cluster node contains child clusters; sibling clusters partition the points covered by their common parent. Hierarchical clustering methods are categorized
into agglomerative (bottom-up) and divisive (top-down). This algorithm is not suitable for Finduber project, because it do not have nested categories.

K-means clustering is a method of vector quantization. It aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. In particular, the parameter k is known to be hard to choose when not given by external constraints. The algorithm has many limitations that make it powerless to solve the clustering problem of Finduber system. One of the limitation is that k-means algorithm cannot be used on non-numeerical data. The algorithm has problems when clusters are of difering sizes and densities, also when the data contains outliters. 

So far, the algorithms that use connectivity models and centroid models are not suitable for solving the data categorization problem defined in Finduber project. Distribution models algorithm is the right option. 

Term frequency is a statistical measure used to evaluate how important a word is to a corpus. The importance increases proportionally to the number of times a word appears in the document. The accuracy of the algorithm depends on the quality of chosen corpus and on the defined feature extraction. 

Text corpus represents a large and structured set of texts that is used to do statistical analysis. To create the corpus for Finduber project it is need to examine the database schema from listing \ref{schema} and to see which records are more relevant. 

\lstinputlisting [language=Ruby, caption={Database Schema}, label=schema]{../src/schema.rb}

Rows: title, description and keywords are chosen for the text corpus. Given that the database has over one million records the corpus set will represent a large data set. Processing and analysing it may take a lot of time. To solve the issue the MapReduce framework is used. 


MapReduce is an implementation for processing and generating lage data sets. It consists of an workflow and a distributed algorithm. 

To write a MapReduce workflow it is need to create two scripts: the map script, and the recude script. 

The map script takes some input data, and maps it to <key, value> 
pairs according to the specifications. For Finduber project, the task is to count word  frequencies in a text, thus the <word, count> will represent <key, value> pairs. The map script, the, would emit a <word, 1> pair for each word in the input stream. Aggregation (i.e. actual counting) is not the responsability of the map script, its goal is to model the data into <key, value> pairs for the reducer to aggregate.

Emitted <key, value> pairs are then ``shuffled", that basically means that pairs with the same key are grouped and passed to a single machine, which will then run the reduce script over them.

The reduce script takes a collection of <key, value> pairs and 
``reduces" them according to the specified reduce script. In Finduber project the task is to count the number of word occurrences to get frequencies. Thus, the reduce script will simply sum the values of the collection of <key, value> pairs which have the same key. The Figure \ref{mapreduce process} illustrates the described scenario for Fnduber project example.


The distributed algorithm oerchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications an data transfers betwen the parts of the system. Python has it own implemenation of the MapReduce distributed computing framework called mincemeat. The framework is: fault tolerant -- clients can join or leave the cluster at any time without affecting the entire process; secure -- it authenticates both ends of every connection, ensuring that only authorized code is executed. Mincemeat is an open sourcedistributed under the MIT Lincese , that is why I used it for Finduber project. 























